## GraphGAN: Graph Representation Learning with Generative Adversial Nets
---
#### Abstract
- The goal of graph representation learning is embed each vertex into a graph into a low-dimensional vector space.
    - generative models that learn the underlyting connectivity distribution in the graph
    - discriminative models that predict the probanility of edge existence between a pair of vertices.
    - GAN: for a given vertex, the generative model tries to fit its underlysing true connectivity distribution over all other vertices and produces "fake" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model.
    - propose 'graph softmax' to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency.
        - link prediction, node classification and recommendation
---
#### Introduction
- Graph representation learning, known as network embedding, aims to represent each vetex in a graph as a low-dimensional vector.
    - which facilitate tasks of network analysis and prediction over vertices and edges.
- Learned embeddings are benefit a wide range of real-world applications
    - link prediction
    - node classification
    - recommendation
    - visualization
    - knowlege graph representation
    - clustering
    - text embedding
    - social network analysis
- graph type
    - weighted graphs
    - directed graphs
    - signed graphs
    - heterogeneous graph
    - attributed graphs
- graph representation learning can be classified into two categories.
    - generative graph representation
        - assume that, for each vertex $v_c$ there exists an underlying true connectivity preference(or relevance distribution) over all other vertices in the graph
        - The edges in the graph can be viewed as observed samples generated by these conditional distributions, and these generative models learn vertex embeddings by maximizaing the likelihood of edges in the graph.
    - discriminative graph representation
        - do not treat edges as generated from an underlying conditional distribution
        - but aim to learn a classifier for predicting the existence of edges directly.
        - Typically, discriminative models consider two vertices $v_i$ and $v_j$ jointly as features, and predict the probability of an edge existing between the two vertices, i.e, $p(edge|(v_i, v_j))$, based on the training data in the graph.
     - GraphGAN aim to train two models during the learning process of GraphGAN: 
         - 1) Generator $G(v|v_c),$ which tries to fit the underlying true connectivity dist
         - 2) Gene